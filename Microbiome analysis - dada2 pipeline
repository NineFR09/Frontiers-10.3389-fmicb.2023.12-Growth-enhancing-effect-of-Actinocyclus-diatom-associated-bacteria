DADA2 pipeline (Divise Amplicon Denoising Algorithm)
Callahan, B. J., McMurdie, P. J., Rosen, M. J., Han, A. W., Johnson, A. J. A., & Holmes, S. P. (2016). DADA2: High-resolution sample inference from Illumina amplicon data. Nature methods, 13(7), 581-583.

Raw Illumina sequences reads of the microbiome data are deposited in NCBI Sequence Read Archive (SRA) under the BioProject PRJNA885573. 

# STEP1: Set Directories and make subfolders for your files

```{r}
library(R.utils);
library(dada2);
library(ShortRead); packageVersion("ShortRead") # dada2 depends on this
library(tidyverse); packageVersion("dplyr") # for manipulating data
library(Biostrings);  # for creating the final graph at the end of the pipeline
library(Hmisc); packageVersion("Hmisc") # for creating the final graph at the end of the pipeline
library(plotly); packageVersion("plotly") # enables creation of interactive graphs, especially helpful for quality plots
library(here); packageVersion("here");
here();
library(readr);
library(Biostrings)
```
```{r}
home.dir <- ("/home/nine/"); #dataset location
setwd(home.dir);
plates<-read_csv('plates');
args = commandArgs(trailingOnly=TRUE);
p=args[1];
p=1
print(p);
plates[p,1];
home.dir <- ("/home/nine/"); #dataset location
base.dir <- (paste0(home.dir, plates[p,1], "/"));
dir.create(paste0(plates[p,1], ".final_EE26.265_255"));
export_dir <-(paste0(plates[p,1], ".final_EE26.265_255/"));
dir.create(paste0(export_dir, "pseudo.run_265_255"));
trimmed_dir <-(paste0(plates[p,1], ".trimmed/"));
dir.create(paste0(trimmed_dir));
trunc_dir <-(paste0(export_dir, "pseudo.run_265_255"));
trimLeng <-(paste0(trimmed_dir, "pseudo.run_265_255"));
dir.create(paste0(trimLeng));
files.fp.gz<-list.files(path=base.dir,  pattern=".fastq.gz");
data.fp.gz <- paste0(plates[p,1], "/");
print(data.fp.gz);

for (i in seq_along (files.fp.gz)){
              gunzip(filename=paste0(data.fp.gz,files.fp.gz[i]), overwrite=T)};
files.fp.gz<-list.files(path=base.dir,  pattern=".fastq");
data.fp.gz <- paste0(plates[p,1], "/");
print(data.fp.gz);
fnFs <- sort(list.files(data.fp.gz, pattern="_R1.fastq", full.names = TRUE)); ## sometimes it will be L001, R1 = forward, R2 = reverse
fnRs <- sort(list.files(data.fp.gz, pattern="_R2.fastq", full.names = TRUE)); 

sample.names <- sapply(strsplit(basename(fnFs), "-"), `[`, 1); ##where you want to separate your sample name ERROR happen here often
sample.names  ## double check they are correct
```

# STEP2: plot quality profiles / primers

```{r}
fwd.plot.quals <- plotQualityProfile(fnFs[1:6])
ggsave(file = paste0(export_dir,"fwd.qualplot.pdf"), fwd.plot.quals, device="pdf");
rev.plot.quals <- plotQualityProfile(fnRs[1:6])
ggsave(file = paste0(export_dir,"rev.qualplot.pdf"), rev.plot.quals, device="pdf");

FWD <- "AGAGTTTGATCMTGGCTCAG";  ## CHANGE: Bacteria 16S: 27f V1_V3
REV <- "GWATTACCGCGGCKGCTG"; ## CHANGE: Bacteria 16S: 519r V1_V3

allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD);
REV.orients <- allOrients(REV);
data.fp <- trimmed_dir;

##filter end to make the other steps faster (we delete all ends after anyway)
fnFs.filtN <- file.path(data.fp, "filtN", basename(fnFs)); # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(data.fp, "filtN", basename(fnRs));
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE, compress=F); ## 0 because good quality seq
passed.filtN <- file.exists(fnFs.filtN) # TRUE/FALSE vector of which samples passed the filter (do we lose any samples)
fnFs.filtN <- fnFs.filtN[passed.filtN] # Keep only those samples that passed the filter
fnFs <- fnFs[passed.filtN] # Keep only those samples that passed the filter
fnRs.filtN <- fnRs.filtN[passed.filtN] # Keep only those samples that passed the filter
fnRs <- fnRs[passed.filtN] # Keep only those samples that passed the filter

primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0));
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]));
```

# STEP3: Cutadapt

```{r}
##make sure the pathways to cutadapt is correct
cutadapt <- "/usr/bin/cutadapt";
data.fp <- paste0(trimmed_dir, "filtN");
path.cut <- file.path(trimmed_dir, "cutadapt");
if(!dir.exists(path.cut)) dir.create(path.cut);
fnFs.cut <- file.path(path.cut, basename(fnFs));
fnRs.cut <- file.path(path.cut, basename(fnRs));
FWD.RC <- dada2:::rc(FWD);
REV.RC <- dada2:::rc(REV);
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC);
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC);
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files Default error rate: 0.1
}
data.fp <- paste0(trimmed_dir, "cutadapt");

##re-check if you have primer and it should be 0 as cutadapt remove them 
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0));
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]));
```

```{r}
#making new sample names based on what went through cutadapt and making new sample (if you lose sample, which should not happen OR with poor quality seqs)
data.fp <- paste0(trimmed_dir, "cutadapt");
passed.fnFs.cut <- file.exists(fnFs.cut) # TRUE/FALSE vector of which samples passed the filter
fnFs.cut <- fnFs.cut[passed.fnFs.cut] # Keep only those samples that passed the filter
fnFs <- fnFs[passed.fnFs.cut] # Keep only those samples that passed the filter
fnRs.cut <- fnRs.cut[passed.fnFs.cut] # Keep only those samples that passed the filter
fnRs <- fnRs[passed.fnFs.cut] # Keep only those samples that passed the filter
sample.names <- sapply(strsplit(basename(fnFs), "-"), `[`, 1);
```

#STEP4: Dada2 trim step 
```{r}
#here you are renaming your new files to be sample name_R1_trim.fastq, no need to change
trimFs <- file.path(trimmed_dir, paste0(sample.names, "-R1_trim.fastq"));
trimRs <- file.path(trimmed_dir, paste0(sample.names, "-R2_trim.fastq"));
names(trimFs) <- sample.names;
names(trimRs) <- sample.names;
head(sample.names);

out <- filterAndTrim(fnFs.cut, trimFs, fnRs.cut, trimRs, truncLen=c(265,255), ##what you trim based on your quality plot
              maxN=0, maxEE=c(6,6), truncQ=6, rm.phix=TRUE, #maxEE = error rate, quality score of each sequence, mistake or not. minimum = 2,2 (standard), 2,6 is pretty good if bad R, highest would be 8,8 (if really bad but not ideal or you let crap in). truncQ is also quality and 2 is good
              compress=FALSE, multithread=10, minLen = 50, matchIDs=TRUE); # On Windows set multithread=FALSE
head(out); ## minLen remove everything less than 50bp, matchIDs = need to match F/R 

saveRDS(out, file = paste0(trunc_dir,"/", "out.RDS"), ascii = FALSE, version = NULL,
        compress = TRUE, refhook = NULL)
```

```{r}
##double checking that all samples passed the trims len we gave earlier and renaming the file with trim on it
passed.trim <- file.exists(trimFs); # TRUE/FALSE vector of which samples passed the filter
trimFs <- trimFs[passed.trim]; # Keep only those samples that passed the filter
trimRs <- trimRs[passed.trim]; # Keep only those samples that passed the filter
data.fp <- paste0(trimLeng);
errF <- learnErrors(trimFs, nbases =1e8, verbose=TRUE, multithread=10, MAX_CONSIST=20);
errR <- learnErrors(trimRs, nbases =1e8, verbose=TRUE, multithread=10, MAX_CONSIST=20);
fwd.plot.errors <- plotErrors(errF, nominalQ=TRUE);
ggsave(file = paste0(trunc_dir,"/","fwd.errors.pdf"), fwd.plot.errors, width = 10, height = 10, device="pdf");
rev.plot.errors <- plotErrors(errR, nominalQ=TRUE);
ggsave(file = paste0(trunc_dir,"/","rev.errors.pdf"), rev.plot.errors, width = 10, height = 10, device="pdf");
```

#STEP5:  DEREPLICATION, DADA2 Step, MERGE, COLLAPSE 

```{r}
## get rid of duplicate / count abundance, making a new list with dereplicated F and R to not merge them several time 
derepFs<-derepFastq(trimFs);
derepRs<-derepFastq(trimRs);
dadaFs <- dada(derepFs, err=errF, multithread=10, pool="pseudo"); ##pseudo="pool" will take random sample to check if ASV are find in several samples before merging 
dadaRs <- dada(derepRs, err=errR, multithread=10, pool="pseudo"); 
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, minOverlap = 12, maxMismatch = 0, verbose=TRUE);
# Inspect the merger data.frame from the first sample
head(mergers[[1]]);
seqtab <- makeSequenceTable(mergers);
dim(seqtab);
saveRDS(seqtab, file = paste0(trunc_dir,"/",plates[p,1],"seqtab.RDS"), ascii = FALSE, version = NULL,
        compress = TRUE, refhook = NULL)
##doing collapse no mismatch AFTER running both plates of sequences
```

# STEP6:  Write tables
```{r}
#changeG information in the following CSVs to match what you used in step 5 so that you have a record of your parameters when you should need it
getN <- function(x) sum(getUniques(x));
table(nchar(getSequences(seqtab)));
seqtab.nochim.1 <- removeBimeraDenovo(seqtab, method="consensus", multithread=10, verbose=TRUE, minFoldParentOverAbundance=1);
dim(seqtab.nochim.1);
sum(seqtab.nochim.1)/sum(seqtab);
track.1 <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim.1));
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track.1) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim.1");
rownames(track.1) <- sample.names;
head(track.1);
track.table1 <- as.data.frame(t(track.1));
track.table1$truncL <- 255;
track.table1$truncR <- 255;
track.table1$EE <- 6.6;
track.table1$truncQ <- 6;
track.table1$pooled <- "pseudo";
track.table1$Bimera <- 1;
track.table1$nochim1_precent <- sum(seqtab.nochim.1)/sum(seqtab);
write.csv(track.table1, file = paste0(trunc_dir,"/",plates[p,1],".trackB1.csv"), col.names=NA);
# tracking reads by percentage
track.1 <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim.1));
colnames(track.1) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim.1");
rownames(track.1) <- sample.names;
head(track.1);
track_pct <- track.1 %>% 
  data.frame() %>%
  mutate(Sample = rownames(.),
         filtered_pct = ifelse(filtered == 0, 0, 100 * (filtered/input)),
         denoisedF_pct = ifelse(denoisedF == 0, 0, 100 * (denoisedF/filtered)),
         denoisedR_pct = ifelse(denoisedR == 0, 0, 100 * (denoisedR/filtered)),
         merged_pct = ifelse(merged == 0, 0, 100 * merged/((denoisedF + denoisedR)/2)),
         nonchim_pct = ifelse(nonchim.1 == 0, 0, 100 * (nonchim.1/merged)),
         total_pct = ifelse(nonchim.1 == 0, 0, 100 * nonchim.1/input)) %>%
  select(Sample, ends_with("_pct"));
track_pct_avg <- track_pct %>% summarize_at(vars(ends_with("_pct")), 
                                            list(avg = mean));
head(track_pct_avg);
track_pct_med <- track_pct %>% summarize_at(vars(ends_with("_pct")), 
                                            list(avg = stats::median));
head(track_pct_avg);
head(track_pct_med);
track_plot.1 <- track.1 %>% 
  data.frame() %>%
  mutate(Sample = rownames(.)) %>%
  gather(key = "Step", value = "Reads", -Sample) %>%
  mutate(Step = factor(Step, 
                       levels = c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim.1"))) %>%
  ggplot(aes(x = Step, y = Reads)) +
  geom_line(aes(group = Sample), alpha = 0.2) +
  geom_point(alpha = 0.5, position = position_jitter(width = 0)) + 
  stat_summary(fun.y = median, geom = "line", group = 1, color = "steelblue", size = 1, alpha = 0.5) +
  stat_summary(fun.y = median, geom = "point", group = 1, color = "steelblue", size = 2, alpha = 0.5) +
  stat_summary(fun.data = median_hilow, fun.args = list(conf.int = 0.5), 
               geom = "ribbon", group = 1, fill = "steelblue", alpha = 0.2) +
  geom_label(data = t(track_pct_avg[1:5]) %>% data.frame() %>% 
               rename(Percent = 1) %>%
               mutate(Step = c("filtered", "denoisedF", "denoisedR", "merged", "nonchim.1"),
                      Percent = paste(round(Percent, 2), "%")),
             aes(label = Percent), y = 1.1 * max(track.1[,2])) +
  geom_label(data = track_pct_avg[6] %>% data.frame() %>%
               rename(total = 1),
             aes(label = paste("Total\nRemaining:\n", round(track_pct_avg[1,6], 2), "%")), 
             y = mean(track.1[,6]), x = 6.5) +
  expand_limits(y = 1.1 * max(track.1[,2]), x = 7) +
  theme_classic();
ggsave(track_plot.1, file = paste0(trunc_dir,"/","track_plot.1.pdf"), width = 10, height = 10, device="pdf")
#TRACK READS THROUGH THE PIPELINE FOR CHIM.1
getN <- function(x) sum(getUniques(x));
track.1 <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim.1));
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track.1) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim.1");
rownames(track.1) <- sample.names;
head(track.1);
track.1 <- as.data.frame(t(track.1));
plotLengthDistro <- function(st) {
  require(ggplot2)
  tot.svs <- table(nchar(colnames(st)))
  tot.reads <- tapply(colSums(st), nchar(colnames(st)), sum)
  df <- data.frame(Length=as.integer(c(names(tot.svs), names(tot.reads))),
                   Count=c(tot.svs, tot.reads),
                   Type=rep(c("SVs", "Reads"), times=c(length(tot.svs), length(tot.reads))))
  pp <- ggplot(data=df, aes(x=Length, y=Count, color=Type)) + geom_point() + facet_wrap(~Type, scales="free_y") + theme_bw() + xlab("Amplicon Length")
  pp
  }
plotLengthDistro(seqtab.nochim.1);
plotLengthDist.log10.chim.1 <-plotLengthDistro(seqtab.nochim.1) + scale_y_log10();
ggsave(plotLengthDist.log10.chim.1, file = paste0(trunc_dir,"/","plotLengthDist.log10.chim.1.pdf"), width = 10, height = 10, device="pdf")
sample <- rownames(seqtab.nochim.1);
sequence <- colnames(seqtab.nochim.1);
#check the col names and check how many ASVs that you are losing in the pipeline
colnames(seqtab.nochim.1);
#what % had chimera's vs non-chimeras?
sum(seqtab.nochim.1)/sum(seqtab);
#this is the %
sum(rev(sort(colSums(seqtab.nochim.1)))[1:1000])/sum(colSums(seqtab.nochim.1));
# Flip table
seqtab.t.1 <- as.data.frame(t(seqtab.nochim.1));
write.csv(seqtab.t.1, file = paste0(trunc_dir,"/",plates[p,1],".ASV.table.chim.1.csv"), col.names=NA);
# tracking reads by percentage
track.1 <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim.1));
colnames(track.1) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim.1");
rownames(track.1) <- sample.names;
head(track.1);
track_pct <- track.1 %>% 
  data.frame() %>%
  mutate(Sample = rownames(.),
         filtered_pct = ifelse(filtered == 0, 0, 100 * (filtered/input)),
         denoisedF_pct = ifelse(denoisedF == 0, 0, 100 * (denoisedF/filtered)),
         denoisedR_pct = ifelse(denoisedR == 0, 0, 100 * (denoisedR/filtered)),
         merged_pct = ifelse(merged == 0, 0, 100 * merged/((denoisedF + denoisedR)/2)),
         nonchim_pct = ifelse(nonchim.1 == 0, 0, 100 * (nonchim.1/merged)),
         total_pct = ifelse(nonchim.1 == 0, 0, 100 * nonchim.1/input)) %>%
  select(Sample, ends_with("_pct"));
track_pct_avg <- track_pct %>% summarize_at(vars(ends_with("_pct")), 
                                            list(avg = mean));
head(track_pct_avg);
track_pct_med <- track_pct %>% summarize_at(vars(ends_with("_pct")), 
                                            list(avg = stats::median));
head(track_pct_avg);
head(track_pct_med);
track_plot.1 <- track.1 %>% 
  data.frame() %>%
  mutate(Sample = rownames(.)) %>%
  gather(key = "Step", value = "Reads", -Sample) %>%
  mutate(Step = factor(Step, 
                       levels = c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim.1"))) %>%
  ggplot(aes(x = Step, y = Reads)) +
  geom_line(aes(group = Sample), alpha = 0.2) +
  geom_point(alpha = 0.5, position = position_jitter(width = 0)) + 
  stat_summary(fun.y = median, geom = "line", group = 1, color = "steelblue", size = 1, alpha = 0.5) +
  stat_summary(fun.y = median, geom = "point", group = 1, color = "steelblue", size = 2, alpha = 0.5) +
  stat_summary(fun.data = median_hilow, fun.args = list(conf.int = 0.5), 
               geom = "ribbon", group = 1, fill = "steelblue", alpha = 0.2) +
  geom_label(data = t(track_pct_avg[1:5]) %>% data.frame() %>% 
               rename(Percent = 1) %>%
               mutate(Step = c("filtered", "denoisedF", "denoisedR", "merged", "nonchim.1"),
                      Percent = paste(round(Percent, 2), "%")),
             aes(label = Percent), y = 1.1 * max(track.1[,2])) +
  geom_label(data = track_pct_avg[6] %>% data.frame() %>%
               rename(total = 1),
             aes(label = paste("Total\nRemaining:\n", round(track_pct_avg[1,6], 2), "%")), 
             y = mean(track.1[,6]), x = 6.5) +
  expand_limits(y = 1.1 * max(track.1[,2]), x = 7) +
  theme_classic();
ggsave(track_plot.1, file = paste0(trunc_dir,"/",plates[p,1],".track_plot.1.pdf"), width = 10, height = 10, device="pdf")
#save RDS files of the seqtab files of importance:
saveRDS(seqtab.nochim.1, file = paste0(trunc_dir,"/",plates[p,1],"seqtab.1.RDS"), ascii = FALSE, version = NULL,
        compress = TRUE, refhook = NULL)
```

```{r}
# Flip table
home.dir <- ("/home/nine/nine.dada2.final_EE26.265_255/pseudo.run_265_255")
setwd(home.dir)
nine.dada2.ASV.table.chim.1 <- read_csv("nine.dada2.ASV.table.chim.1.csv")
names(nine.dada2.ASV.table.chim.1)[names(nine.dada2.ASV.table.chim.1) == "X1"] <- "ASV"
nine_ASVtable <- nine.dada2.ASV.table.chim.1 %>% gather(key='code', value='abund', -c('ASV'))
#We are going to now name the ASVs with identifying numbers
nine.ref <- nine_ASVtable %>%  group_by(ASV) %>% dplyr::summarise (asvT=sum(abund)) %>% arrange(desc(asvT))
nine.ref$id <- seq(1000001, 1000000+nrow(nine.ref), 1)
nine.ref$prefix <- 'B'
nine.ref <- nine.ref %>% unite('ASVid', c(prefix,id), sep='', remove=T)
nine.ref <- nine.ref %>% select(ASV,ASVid)
#write_csv(nine.ref,"ASV_list_taxa.csv")
nine  <- nine_ASVtable  %>% left_join(nine.ref, 'ASV')
nine.sample.sum <- nine %>% group_by(code) %>% dplyr::mutate(sample_total=sum(abund))
```

## Collapse the 4 file together
```{r}
taxaSilva138<-list.files(pattern='taxaSilva138.part.')
taxaSilva138.list<-list('vector', length(taxaSilva138))
taxaSilva138.list<- lapply(taxaSilva138, function(x) read_csv(x))
taxaSilva138.bind<-list('vector', length(taxaSilva138.list))
for (i in seq_along(taxaSilva138.list)){
  taxaSilva138.bind[[i]]<- taxaSilva138.list[[i]] %>% mutate('platecode'=names(taxaSilva138.list)[i])
}
taxaSilva138.bind<-bind_rows(taxaSilva138.bind[], .id = "column_label")
taxaSilva138.bind$tax.Kingdom <- forcats::fct_explicit_na(taxaSilva138.bind$tax.Kingdom, 'k_unassigned')
taxaSilva138.bind$tax.Phylum <- forcats::fct_explicit_na(taxaSilva138.bind$tax.Phylum, 'p_unassigned')
taxaSilva138.bind$tax.Class <- forcats::fct_explicit_na(taxaSilva138.bind$tax.Class, 'c_unassigned')
taxaSilva138.bind$tax.Order <- forcats::fct_explicit_na(taxaSilva138.bind$tax.Order, 'o_unassigned')
taxaSilva138.bind$tax.Family <- forcats::fct_explicit_na(taxaSilva138.bind$tax.Family, 'f_unassigned')
taxaSilva138.bind$tax.Genus <- forcats::fct_explicit_na(taxaSilva138.bind$tax.Genus, 'g_unassigned')
#write the bind file which is all of the tax files bound together as one CSV file
write_csv(taxaSilva138.bind,'tax_1_Long2021may11.silva138.csv')
```
```{r}
tax_1_Long2021may11_silva138 <- read_csv("~/PHD/Microbiome_Actinocyclus/taxa/tax_1_Long2021may11.silva138.csv")
```
